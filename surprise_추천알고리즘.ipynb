{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1izbg3rOa7JcfqjLNTCgk17gHb4VYObXA",
      "authorship_tag": "ABX9TyMx/fydjxKfOA06iq5ZlPzN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/standha/Surprise-Recommendation-Algorithm/blob/main/surprise_%EC%B6%94%EC%B2%9C%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IZyxufhAqm2d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1X2EDDkUQ-ti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bac29ef-3692-4865-ea06-6877d77ff074"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/96.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m92.2/96.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q tensorflow-recommenders\n",
        "!pip install -q --upgrade tensorflow-datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_recommenders as tfrs"
      ],
      "metadata": {
        "id": "UTNXpMfBRpE2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# 'tbl_post.csv'에서 'bno'와 'userid' 열을 선택\n",
        "post_df = pd.read_csv('tbl_post.csv')\n",
        "post_df['userid'] = post_df['userid'].astype(str)\n",
        "post_ratings = post_df[['bno', 'userid']]\n",
        "\n",
        "# 'tbl_board.csv'에서 'bno'와 'title' 열을 선택\n",
        "board_df = pd.read_csv('tbl_board.csv')\n",
        "board_df['title'] = board_df['title'].astype(str)\n",
        "board_data = board_df[['bno', 'title']]\n",
        "\n",
        "# 'bno'를 기준으로 두 데이터 프레임을 병합\n",
        "merged_data = post_ratings.merge(board_data, on='bno', how='inner')\n",
        "\n",
        "# # 필요한 열 선택\n",
        "ratings = merged_data[['title', 'userid']]\n",
        "movies = board_data[['title']]\n",
        "\n",
        "\n",
        "def pandas_to_dataset(dataframe, target_column_name):\n",
        "    dataframe = dataframe.copy()\n",
        "    labels = dataframe.pop(target_column_name)  # 타겟 열 선택\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "    return dataset\n",
        "\n",
        "# ratings 및 movies 데이터프레임을 TensorFlow Datasets로 변환\n",
        "# Create 'ratings_dataset' with 1D tensors\n",
        "# ratings_dataset = tf.data.Dataset.from_tensor_slices((merged_data['userid'], merged_data['title']))\n",
        "\n",
        "# # Create 'movies_dataset' using the 'title' column from 'movies' DataFrame\n",
        "# movies_dataset = tf.data.Dataset.from_tensor_slices(movies['title'])\n",
        "ratings_dataset = pandas_to_dataset(ratings,'userid')\n",
        "movies_dataset = pandas_to_dataset(movies, 'title')\n",
        "\n",
        "# Create a function to convert a row to the desired format\n",
        "def map_features(x,y):\n",
        "    return {\n",
        "        \"title\": x[\"title\"],\n",
        "        \"userid\": y\n",
        "    }\n",
        "\n",
        "# # Apply the mapping function to each row in the dataset\n",
        "ratings_dataset = ratings_dataset.map(map_features)\n",
        "movies_dataset = tf.data.Dataset.from_tensor_slices(movies['title'])\n",
        "# movies_dataset = tf.data.Dataset.from_tensor_slices(movies['title']).batch(128)\n",
        "\n",
        "# print(len(list(ratings_dataset)))\n",
        "print(len(list(movies_dataset)))\n",
        "len(list(movies_dataset))\n",
        "# len(list(ratings_dataset))"
      ],
      "metadata": {
        "id": "VLzV_1i9O9_6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b144f8d2-8c7c-4a1f-9659-264fedc6dbff"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1182\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1182"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_ids_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
        "user_ids_vocabulary.adapt(ratings_dataset.map(lambda x : x['userid']))\n",
        "\n",
        "movie_titles_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
        "movie_titles_vocabulary.adapt(movies_dataset)"
      ],
      "metadata": {
        "id": "mtpn0L-77-RZ"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MovieLensModel(tfrs.Model):\n",
        "  # We derive from a custom base class to help reduce boilerplate. Under the hood,\n",
        "  # these are still plain Keras Models.\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      user_model: tf.keras.Model,\n",
        "      movie_model: tf.keras.Model,\n",
        "      task: tfrs.tasks.Retrieval):\n",
        "    super().__init__()\n",
        "\n",
        "    # Set up user and movie representations.\n",
        "    self.user_model = user_model\n",
        "    self.movie_model = movie_model\n",
        "\n",
        "    # Set up a retrieval task.\n",
        "    self.task = task\n",
        "\n",
        "\n",
        "\n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "    # Define how the loss is computed.\n",
        "\n",
        "    user_embeddings = self.user_model(features[\"userid\"])\n",
        "    movie_embeddings = self.movie_model(features[\"title\"])\n",
        "\n",
        "    return self.task(user_embeddings, movie_embeddings)\n",
        "\n",
        "\n",
        "# class MovieLensModel(tfrs.Model):\n",
        "#     def __init__(self, user_model: tf.keras.Model, movie_model: tf.keras.Model, task: tfrs.tasks.Retrieval):\n",
        "#         super(MovieLensModel, self).__init__()\n",
        "#         self.user_model = user_model\n",
        "#         self.movie_model = movie_model\n",
        "#         self.task = task\n",
        "\n",
        "#     def call(self, features):\n",
        "#         # 구현하려는 연산을 여기에 추가\n",
        "#         user_embeddings = self.user_model(features[\"userid\"])\n",
        "#         movie_embeddings = self.movie_model(features[\"title\"])\n",
        "#         return user_embeddings, movie_embeddings\n",
        "\n",
        "#     def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "#         user_embeddings, movie_embeddings = self.call(features)\n",
        "#         return self.task(user_embeddings, movie_embeddings)\n",
        "\n"
      ],
      "metadata": {
        "id": "rkPq887GIH5g"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ratings 데이터셋 확인\n",
        "for sample in ratings_dataset.take(1):\n",
        "    print(sample)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AE23Hacc6MI",
        "outputId": "3dc86deb-27b5-4b41-df8d-5fdc90375da5"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'title': <tf.Tensor: shape=(), dtype=string, numpy=b'\\xed\\x8c\\x8c \\xeb\\xb0\\x94\\xea\\xb2\\x8c\\xed\\x8a\\xb8'>, 'userid': <tf.Tensor: shape=(), dtype=string, numpy=b'20204012'>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define user and movie models.\n",
        "# user_model = tf.keras.Sequential([\n",
        "#     user_ids_vocabulary,\n",
        "#     tf.keras.layers.Embedding(user_ids_vocabulary.vocabulary_size(), 64)\n",
        "# ])\n",
        "# movie_model = tf.keras.Sequential([\n",
        "#     movie_titles_vocabulary,\n",
        "#     tf.keras.layers.Embedding(movie_titles_vocabulary.vocabulary_size(), 64)\n",
        "# ])\n",
        "\n",
        "user_model = tf.keras.Sequential([\n",
        "    user_ids_vocabulary,\n",
        "    tf.keras.layers.Embedding(input_dim=user_ids_vocabulary.vocab_size(), output_dim=64)\n",
        "])\n",
        "movie_model = tf.keras.Sequential([\n",
        "    movie_titles_vocabulary,\n",
        "    tf.keras.layers.Embedding(input_dim=movie_titles_vocabulary.vocab_size(), output_dim=64)\n",
        "])\n",
        "\n",
        "# user_model = tf.keras.Sequential([\n",
        "#     user_ids_vocabulary,\n",
        "#     tf.keras.layers.Embedding(user_ids_vocabulary.vocab_size(), 64)\n",
        "# ])\n",
        "# movie_model = tf.keras.Sequential([\n",
        "#     movie_titles_vocabulary,\n",
        "#     tf.keras.layers.Embedding(movie_titles_vocabulary.vocab_size(), 64)\n",
        "# ])\n",
        "\n",
        "# Define your objectives.\n",
        "task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(\n",
        "    movies_dataset.batch(128).map(movie_model)\n",
        "))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIdGe2i9IQpc",
        "outputId": "7561bb63-f252-40ad-e136-bdf2fea380d3"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n",
            "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ratings_dataset 출력\n",
        "for features, label in ratings_dataset.take(5):\n",
        "    print(f'Features: {features}, Label: {label}')\n",
        "\n",
        "# movies_dataset 출력\n",
        "for title in movies_dataset.take(5):\n",
        "    print(f'Title: {title}')\n",
        "    # movies_dataset의 모양과 데이터 타입 확인\n",
        "    print(title.shape, title.dtype)\n",
        "\n",
        "# ratings_dataset 출력\n",
        "for userid in ratings_dataset.take(5):\n",
        "    print(f'Userid: {userid}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tf.print(title)\n",
        "tf.print(userid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SEUWx4V8wAr",
        "outputId": "172828d7-90e0-49b9-bafc-9c96b74fe45d"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features: title, Label: userid\n",
            "Features: title, Label: userid\n",
            "Features: title, Label: userid\n",
            "Features: title, Label: userid\n",
            "Features: title, Label: userid\n",
            "Title: b'\\xeb\\x82\\x98\\xeb\\xac\\xbc\\xeb\\xb9\\x84\\xeb\\xb9\\x94\\xeb\\xb0\\xa5'\n",
            "() <dtype: 'string'>\n",
            "Title: b'\\xec\\x98\\xa4\\xea\\xb3\\xa1\\xeb\\xb0\\xa5'\n",
            "() <dtype: 'string'>\n",
            "Title: b'\\xec\\x9e\\xa1\\xec\\xb1\\x84\\xeb\\xb0\\xa5'\n",
            "() <dtype: 'string'>\n",
            "Title: b'\\xec\\xbd\\xa9\\xeb\\x82\\x98\\xeb\\xac\\xbc\\xeb\\xb0\\xa5'\n",
            "() <dtype: 'string'>\n",
            "Title: b'\\xec\\x95\\xbd\\xec\\x8b\\x9d'\n",
            "() <dtype: 'string'>\n",
            "Userid: {'title': <tf.Tensor: shape=(), dtype=string, numpy=b'\\xed\\x8c\\x8c \\xeb\\xb0\\x94\\xea\\xb2\\x8c\\xed\\x8a\\xb8'>, 'userid': <tf.Tensor: shape=(), dtype=string, numpy=b'20204012'>}\n",
            "Userid: {'title': <tf.Tensor: shape=(), dtype=string, numpy=b'\\xed\\x8c\\x8c \\xeb\\xb0\\x94\\xea\\xb2\\x8c\\xed\\x8a\\xb8'>, 'userid': <tf.Tensor: shape=(), dtype=string, numpy=b'20204012'>}\n",
            "Userid: {'title': <tf.Tensor: shape=(), dtype=string, numpy=b'\\xed\\x8c\\x8c \\xeb\\xb0\\x94\\xea\\xb2\\x8c\\xed\\x8a\\xb8'>, 'userid': <tf.Tensor: shape=(), dtype=string, numpy=b'20204012'>}\n",
            "Userid: {'title': <tf.Tensor: shape=(), dtype=string, numpy=b'\\xed\\x8c\\x8c \\xeb\\xb0\\x94\\xea\\xb2\\x8c\\xed\\x8a\\xb8'>, 'userid': <tf.Tensor: shape=(), dtype=string, numpy=b'20204012'>}\n",
            "Userid: {'title': <tf.Tensor: shape=(), dtype=string, numpy=b'\\xed\\x8c\\x8c \\xeb\\xb0\\x94\\xea\\xb2\\x8c\\xed\\x8a\\xb8'>, 'userid': <tf.Tensor: shape=(), dtype=string, numpy=b'20204012'>}\n",
            "약식\n",
            "{'title': \"파 바게트\", 'userid': \"20204012\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a retrieval model.\n",
        "model = MovieLensModel(user_model, movie_model, task)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\n",
        "\n",
        "# Train for 3 epochs.\n",
        "model.fit(ratings_dataset.batch(4096), epochs=3)\n",
        "userid = \"2020412\"\n",
        "##해당 아이디가 추천을 남긴 게시글 제외\n",
        "# def get_rated_movies(userid, ratings_dataset):\n",
        "#     return list(ratings_dataset[ratings_dataset['userid'] == userid]['title'])\n",
        "# user_rated_movies = get_rated_movies(userid,ratings_dataset)\n",
        "# all_movie_titles = list(movies_dataset['title'])\n",
        "# recommended_movies = [title for title in all_movie_titles if title not in user_rated_movies]\n",
        "\n",
        "# merged_data에서 받아온 userid가 작성한 title을 가져옵니다.\n",
        "user_titles = merged_data[merged_data['userid'] == userid]['title'].unique()\n",
        "\n",
        "# movies_dataset을 필터링하여 user_titles와 동일하지 않은 title만 남깁니다.\n",
        "movies_dataset = movies_dataset.filter(lambda title: tf.math.logical_not(tf.math.reduce_any(tf.math.equal(title, user_titles))))\n",
        "\n",
        "\n",
        "# movies 데이터프레임에서 해당 title을 제외합니다.\n",
        "# movies = movies[~movies['title'].isin(user_titles)]\n",
        "\n",
        "\n",
        "\n",
        "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
        "index.index_from_dataset(\n",
        "    movies_dataset.batch(100).map(lambda title: (title, model.movie_model(title))))\n",
        "\n",
        "\n",
        "# Get some recommendations.\n",
        "_, titles = index(np.array([userid], dtype=object))\n",
        "titles = [title.numpy().decode('utf-8') for title in titles[0, :5]]\n",
        "print(f\"Top 3 recommendations for user {userid}: {titles}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8v7qmF-MKHz",
        "outputId": "6f440b25-b789-43b5-b628-f17f3f638568"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1/1 [==============================] - 1s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.7692 - factorized_top_k/top_5_categorical_accuracy: 0.9615 - factorized_top_k/top_10_categorical_accuracy: 0.9615 - factorized_top_k/top_50_categorical_accuracy: 0.9615 - factorized_top_k/top_100_categorical_accuracy: 0.9615 - loss: 125.9855 - regularization_loss: 0.0000e+00 - total_loss: 125.9855\n",
            "Epoch 2/3\n",
            "1/1 [==============================] - 0s 144ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0769 - factorized_top_k/top_5_categorical_accuracy: 1.0000 - factorized_top_k/top_10_categorical_accuracy: 1.0000 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 495.5263 - regularization_loss: 0.0000e+00 - total_loss: 495.5263\n",
            "Epoch 3/3\n",
            "1/1 [==============================] - 0s 72ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.9231 - factorized_top_k/top_10_categorical_accuracy: 0.9231 - factorized_top_k/top_50_categorical_accuracy: 0.9231 - factorized_top_k/top_100_categorical_accuracy: 0.9231 - loss: 135.6908 - regularization_loss: 0.0000e+00 - total_loss: 135.6908\n",
            "Top 3 recommendations for user 2020412: ['닭다리굴소스볶음', '홍합탕', '닭고기볶음밥', '영양밥', '오징어불고기']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a retrieval model.\n",
        "model = MovieLensModel(user_model, movie_model, task)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\n",
        "\n",
        "# Train for 3 epochs.\n",
        "model.fit(ratings_dataset.batch(4096), epochs=3)\n",
        "\n",
        "# 사용자 아이디\n",
        "userid = \"csy4012\"\n",
        "\n",
        "# 해당 아이디가 후기를 남긴 title 가져오는 함수\n",
        "def get_rated_movies(userid, ratings_dataset):\n",
        "    user_rated_movies = []\n",
        "    for element in ratings_dataset.as_numpy_iterator():\n",
        "        if element['userid'] == userid:\n",
        "            user_rated_movies.append(element['title'].decode('utf-8'))\n",
        "    return user_rated_movies\n",
        "\n",
        "# 사용자가 이미 평가한 영화 가져오기\n",
        "user_rated_movies = get_rated_movies(userid, ratings_dataset)\n",
        "\n",
        "# 모든 영화 title 가져오기\n",
        "all_movie_titles = list(movies['title'])\n",
        "\n",
        "# 추천을 받아오려는 사용자의 아이디를 제외하고 추천 목록 생성\n",
        "recommended_movies = [title for title in all_movie_titles if title != userid and title not in user_rated_movies]\n",
        "\n",
        "# Get some recommendations.\n",
        "_, titles = index(np.array([userid], dtype=object))\n",
        "titles = [title.numpy().decode('utf-8') for title in titles[0, :3]]\n",
        "print(f\"Top 3 recommendations for user {userid}: {titles}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkJHKF5NDInF",
        "outputId": "403a9398-5165-4af2-c67e-f0858f738e74"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1/1 [==============================] - 3s 3s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.9231 - factorized_top_k/top_10_categorical_accuracy: 0.9231 - factorized_top_k/top_50_categorical_accuracy: 0.9231 - factorized_top_k/top_100_categorical_accuracy: 0.9231 - loss: 100.1339 - regularization_loss: 0.0000e+00 - total_loss: 100.1339\n",
            "Epoch 2/3\n",
            "1/1 [==============================] - 0s 73ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.9615 - factorized_top_k/top_10_categorical_accuracy: 0.9615 - factorized_top_k/top_50_categorical_accuracy: 0.9615 - factorized_top_k/top_100_categorical_accuracy: 0.9615 - loss: 123.9682 - regularization_loss: 0.0000e+00 - total_loss: 123.9682\n",
            "Epoch 3/3\n",
            "1/1 [==============================] - 0s 80ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0769 - factorized_top_k/top_5_categorical_accuracy: 0.9615 - factorized_top_k/top_10_categorical_accuracy: 0.9615 - factorized_top_k/top_50_categorical_accuracy: 0.9615 - factorized_top_k/top_100_categorical_accuracy: 0.9615 - loss: 488.1502 - regularization_loss: 0.0000e+00 - total_loss: 488.1502\n",
            "Top 3 recommendations for user csy4012: ['낙지볶음', '잣죽', '김치동그랑땡']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text = b'\\xeb\\x82\\x99\\xec\\xa7\\x80\\xeb\\xb3\\xb6\\xec\\x9d\\x8c'\n",
        "decoded_text = encoded_text.decode('utf-8')\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6V9hN0Q3DE8",
        "outputId": "22fac338-1a91-4ed0-a13d-b5c7ceeaee89"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "낙지볶음\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import mysql.connector\n",
        "# import requests\n",
        "# import json\n",
        "# from datetime import datetime as datetime\n",
        "# from bs4 import BeautifulSoup\n",
        "\n",
        "# # 함수를 통해 음식 정보 크롤링하여 DB에 저장\n",
        "# def save_food_info_to_db(name, ingredients, recipe):\n",
        "#     try:\n",
        "#         # 데이터베이스 연결\n",
        "#         conn = mysql.connect(\n",
        "#             user=\"kawa\",\n",
        "#             password=\"0000\",\n",
        "#             host=\"localhost\",\n",
        "#             port=3307,\n",
        "#             database=\"board\"\n",
        "#         )\n",
        "#         # 커서 생성\n",
        "#         cursor = conn.cursor()\n",
        "\n",
        "#         #날짜 생성\n",
        "#         now = datetime.now()\n",
        "#         date = now.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "#         name = name\n",
        "#         ingredients = ingredients\n",
        "#         recipe = recipe\n",
        "\n",
        "# #         cursor.execute('SELECT bno FROM tbl_board')\n",
        "# #         id = cursor.fetchone()+1\n",
        "\n",
        "\n",
        "#         # SQL 쿼리 실행하여 데이터베이스에 저장\n",
        "#         cursor.execute(\n",
        "#             f\"INSERT INTO tbl_board(bno, title, content, ingredients, writer, regDate, viewCnt, likeCnt, replyCnt, imgPath, status, category, level, averagerating) VALUES ('26', \\\"{name}\\\", \\\"{recipe}\\\", \\\"{ingredients}\\\", '7', \\\"{date}\\\", 0, 0, 0, 'Fooriend.png', 'Y', '한식', '비건입문자', '2.5')\"\n",
        "#         )\n",
        "\n",
        "\n",
        "#         # 변경사항 커밋\n",
        "#         conn.commit()\n",
        "\n",
        "#     except mysql.Error as e:\n",
        "#         print(f\"An error occurred: {e}\")\n",
        "\n",
        "#     finally:\n",
        "#         # 연결 종료\n",
        "#         if conn:\n",
        "#             conn.close()"
      ],
      "metadata": {
        "id": "2OI3QtLx4jC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mysql.connector\n",
        "\n",
        "# MariaDB 연결 정보 설정\n",
        "host = 'localhost'  # 호스트 정보\n",
        "user = 'kawa'       # 사용자 이름\n",
        "password = '0000'   # 암호\n",
        "database = 'board'  # 데이터베이스 이름\n",
        "port = 3307         # 포트 번호\n",
        "\n",
        "try:\n",
        "  # 연결 생성\n",
        "  conn = mysql.connector.connect(\n",
        "    host=host,\n",
        "    user=user,\n",
        "    password=password,\n",
        "    database=database,\n",
        "    port=port\n",
        "  )\n",
        "\n",
        "# 커서 생성\n",
        "  cursor = conn.cursor()\n",
        "\n",
        "# SQL 쿼리 실행 예시\n",
        "  query = \"SELECT * FROM tbl_board\"\n",
        "  cursor.execute(query)\n",
        "\n",
        "# 결과 가져오기\n",
        "  result = cursor.fetchall()\n",
        "\n",
        "# 연결 종료\n",
        "  cursor.close()\n",
        "  conn.close()\n",
        "except mysql.connector.Error as e:\n",
        "    # 예외가 발생하면 연결 실패 메시지를 출력\n",
        "    print(f\"연결 실패: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYJSb7Su--F0",
        "outputId": "6c5b39a9-0bf8-4739-e6d0-9ed90ca17cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "연결 실패: 2003: Can't connect to MySQL server on 'localhost:3307' (111 Connection refused)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mysql-connector-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmOJOQGQ5q-d",
        "outputId": "77bff99d-bf14-4abd-b316-6f4d341cb0ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mysql-connector-python\n",
            "  Downloading mysql_connector_python-8.1.0-cp310-cp310-manylinux_2_17_x86_64.whl (27.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf<=4.21.12,>=4.21.1 (from mysql-connector-python)\n",
            "  Downloading protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.8/409.8 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, mysql-connector-python\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.21.12 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mysql-connector-python-8.1.0 protobuf-4.21.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf==3.20.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "cWV28oFj6ASO",
        "outputId": "b978d35c-abfa-40dd-c38a-362b2fc4264b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting protobuf==3.20.3\n",
            "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.21.12\n",
            "    Uninstalling protobuf-4.21.12:\n",
            "      Successfully uninstalled protobuf-4.21.12\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mysql-connector-python 8.1.0 requires protobuf<=4.21.12,>=4.21.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mysql-connector-python==8.0.26"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOOVEjsy88tx",
        "outputId": "7433fadf-f0e8-497e-8644-4c8dba39eeb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mysql-connector-python==8.0.26\n",
            "  Downloading mysql_connector_python-8.0.26-py2.py3-none-any.whl (324 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/324.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/324.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.8/324.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from mysql-connector-python==8.0.26) (3.20.3)\n",
            "Installing collected packages: mysql-connector-python\n",
            "  Attempting uninstall: mysql-connector-python\n",
            "    Found existing installation: mysql-connector-python 8.1.0\n",
            "    Uninstalling mysql-connector-python-8.1.0:\n",
            "      Successfully uninstalled mysql-connector-python-8.1.0\n",
            "Successfully installed mysql-connector-python-8.0.26\n"
          ]
        }
      ]
    }
  ]
}